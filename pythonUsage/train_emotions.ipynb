{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#图像读取库\n",
    "from PIL import Image\n",
    "#矩阵运算库\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'happy', 'sad']\n",
      "{'anger': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'sad': 4}\n"
     ]
    }
   ],
   "source": [
    "# 分类\n",
    "label_names = [\"anger\", \"disgust\", \"fear\", \"happy\", \"sad\"]\n",
    "label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
    "\n",
    "print(label_names)\n",
    "print(label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练的图片列表和分类标签信息\n",
    "def load_images(data_dir):\n",
    "    fpaths = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in os.listdir(data_dir):\n",
    "        labelPath = os.path.join(data_dir, label)\n",
    "        for fname in os.listdir(labelPath):\n",
    "            # 文件名\n",
    "            fpath = os.path.join(labelPath, fname)\n",
    "            fpaths.append(fpath)\n",
    "            \n",
    "            # 标签\n",
    "            labels.append(label_to_index[label])\n",
    "    \n",
    "    # 将图片名和标签 打包成一组，然后进行乱序\n",
    "    temp = np.array([fpaths,labels])\n",
    "    temp = temp.transpose()\n",
    "    np.random.shuffle(temp)\n",
    "\n",
    "    fpaths = list(temp[:,0])\n",
    "    labels = list(temp[:,1])\n",
    "    labels = [int(i) for i in labels]\n",
    "    \n",
    "    return fpaths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 79 images\n"
     ]
    }
   ],
   "source": [
    "# 数据文件夹\n",
    "data_dir = \"/home/wilson/workstation/TFResFaceEmotion/emotions\"\n",
    "\n",
    "all_image_paths, all_image_labels = load_images(data_dir)\n",
    "\n",
    "image_count = len(all_image_paths)\n",
    "print(\"total\", image_count, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image,dtype=tf.float32)\n",
    "    image /= 255.0\n",
    "    #print(333, image.shape, image)\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = all_image_paths[0]\n",
    "label = all_image_labels[0]\n",
    "\n",
    "plt.imshow(load_and_preprocess_image(image_path))\n",
    "plt.grid(False)\n",
    "plt.title(label_names[label].title())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANcAAADXCAYAAACJfcS1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAClklEQVR4nO3YsQ0CMRAAwTf6Pui/LHJ6OHIEwUssCJgJbQeXrGx5zcwGvN7p0wPArxIXRMQFEXFBRFwQERdE9iOH11r+7eHOzKxH624uiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggsh88f9227VIMAl/q/Gxjzcw7B4G/4VkIEXFBRFwQERdExAURcUFEXBARF0TEBZEb+kUNl0XHGC8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABUUlEQVR4nO3WuwkDMRBAwZNxC47df1nOr4d1bpwc+PuYCRcFCw8JrZnZ6Dh9ewFeS9AYQWMEjRE05nzk8FrLl/h37DNzeRy6of/r9mwoaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaMz54Pl927bbOxbhsOuz4ZqZTy/CG3lyYwSNETRG0BhBYwSNETRG0BhBY+57+w6yhMa/JQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABUUlEQVR4nO3WuwkDMRBAwZNxC47df1nOr4d1bpwc+PuYCRcFCw8JrZnZ6Dh9ewFeS9AYQWMEjRE05nzk8FrLl/h37DNzeRy6of/r9mwoaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaMz54Pl927bbOxbhsOuz4ZqZTy/CG3lyYwSNETRG0BhBYwSNETRG0BhBY+57+w6yhMa/JQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABUUlEQVR4nO3WuwkDMRBAwZNxC47df1nOr4d1bpwc+PuYCRcFCw8JrZnZ6Dh9ewFeS9AYQWMEjRE05nzk8FrLl/h37DNzeRy6of/r9mwoaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaMz54Pl927bbOxbhsOuz4ZqZTy/CG3lyYwSNETRG0BhBYwSNETRG0BhBY+57+w6yhMa/JQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for n, image in enumerate(image_ds.take(4)):\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.imshow(image)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "anger\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "anger\n",
      "happy\n",
      "anger\n",
      "fear\n",
      "fear\n"
     ]
    }
   ],
   "source": [
    "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64))\n",
    "\n",
    "for label in label_ds.take(10):\n",
    "    print(label_names[label.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ZipDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n",
      "<MapDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# 由于这些数据集顺序相同，你可以将他们打包在一起得到一个(图片, 标签)对数据集：\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "print(image_label_ds)\n",
    "\n",
    "# 元组被解压缩到映射函数的位置参数中\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "    return load_and_preprocess_image(path), label\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "image_label_ds = ds.map(load_and_preprocess_from_path_label)\n",
    "print(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "<PrefetchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"训练的基本方法\n",
    "要使用此数据集训练模型，你将会想要数据：\n",
    "\n",
    "    被充分打乱。\n",
    "    被分割为 batch。\n",
    "    永远重复。\n",
    "    尽快提供 batch。\n",
    "\n",
    "使用 tf.data api 可以轻松添加这些功能。\n",
    "\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 设置一个和数据集大小一致的 shuffle buffer size（随机缓冲区大小）以保证数据\n",
    "# 被充分打乱。\n",
    "ds = image_label_ds.shuffle(buffer_size=image_count)\n",
    "ds = ds.repeat()\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "# 当模型在训练的时候，`prefetch` 使数据集在后台取得 batch。\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "print(ds)\n",
    "\n",
    "ds = image_label_ds.apply(\n",
    "  tf.data.experimental.shuffle_and_repeat(buffer_size=image_count))\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在准备分类模型\n",
    "mobile_net = tf.keras.applications.MobileNetV2(input_shape=(192, 192, 3), include_top=False)\n",
    "mobile_net.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该模型期望它的输出被标准化至 [-1,1] 范围内：\n",
    "# help(tf.keras.applications.mobilenet_v2.preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 6, 6, 1280)\n"
     ]
    }
   ],
   "source": [
    "# 在你将输出传递给 MobilNet 模型之前，你需要将其范围从 [0,1] 转化为 [-1,1]：\n",
    "def change_range(image,label):\n",
    "    return 2*image-1, label\n",
    "\n",
    "keras_ds = ds.map(change_range)\n",
    "\n",
    "# 数据集可能需要几秒来启动，因为要填满其随机缓冲区。\n",
    "image_batch, label_batch = next(iter(keras_ds))\n",
    "\n",
    "feature_map_batch = mobile_net(image_batch)\n",
    "print(feature_map_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    mobile_net,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(len(label_names), activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min logit: 0.105850965\n",
      "max logit: 0.3818455\n",
      "\n",
      "Shape: (32, 5)\n"
     ]
    }
   ],
   "source": [
    "logit_batch = model(image_batch).numpy()\n",
    "\n",
    "print(\"min logit:\", logit_batch.min())\n",
    "print(\"max logit:\", logit_batch.max())\n",
    "print()\n",
    "\n",
    "print(\"Shape:\", logit_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_192 (Functi (None, 6, 6, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch=tf.math.ceil(len(all_image_paths)/BATCH_SIZE).numpy()\n",
    "steps_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 15ms/step - loss: 1.6127 - accuracy: 0.2604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f94ac540250>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds, epochs=1, steps_per_epoch=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练的图片列表和分类标签信息\n",
    "def read_data(data_dir):\n",
    "    fpaths = []\n",
    "    datas = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in os.listdir(data_dir):\n",
    "        labelPath = os.path.join(data_dir, label)\n",
    "        for fname in os.listdir(labelPath):\n",
    "            # 文件名\n",
    "            fpath = os.path.join(labelPath, fname)\n",
    "            fpaths.append(fpath)\n",
    "            \n",
    "            # 图片数据\n",
    "            image = Image.open(fpath)\n",
    "            data = np.array(image) / 255.0\n",
    "            datas.append(data)\n",
    "            \n",
    "            # 标签\n",
    "            labels.append(CATEGORIES[label])\n",
    "    \n",
    "    datas = np.array(datas)\n",
    "    labels = np.array(labels)\n",
    "    print(\"shape of datas :\", datas.shape)\n",
    "    print(\"shape of labels:\", labels.shape)\n",
    "    return fpaths, datas, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths, datas, labels = read_data(data_dir)\n",
    "\n",
    "# 计算有多少类图片\n",
    "num_classes = len(set(labels))\n",
    "print(\"classes:%d\" %num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Placeholder，存放输入和标签\n",
    "datas_placeholder = tf.compat.v1.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "labels_placeholder = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "\n",
    "# 存放DropOut参数的容器，训练时为0.25，测试时为0\n",
    "dropout_placeholdr = tf.compat.v1.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积层, 20个卷积核, 卷积核大小为5，用Relu激活\n",
    "conv0 = tf.layers.conv2d(datas_placeholder, 20, 5, activation=tf.nn.relu)\n",
    "# 定义max-pooling层，pooling窗口为2x2，步长为2x2\n",
    "pool0 = tf.layers.max_pooling2d(conv0, [2, 2], [2, 2])\n",
    "\n",
    "# 定义卷积层, 40个卷积核, 卷积核大小为4，用Relu激活\n",
    "conv1 = tf.layers.conv2d(pool0, 40, 4, activation=tf.nn.relu)\n",
    "# 定义max-pooling层，pooling窗口为2x2，步长为2x2\n",
    "pool1 = tf.layers.max_pooling2d(conv1, [2, 2], [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(image, label, image_W, image_H, batch_size, capacity):\n",
    "    image = tf.cast(image, tf.string)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    \n",
    "    # 加入队列\n",
    "    # input_queue = tf.train.slice_input_producer([image, label])\n",
    "    # input_queue = tf.data.Dataset.from_tensor_slices((image, labels))\n",
    "    input_queue = tf.compat.v1.train.slice_input_producer([image, label])\n",
    "    \n",
    "    label = input_queue[1]\n",
    "    \n",
    "    # 将图片按照jpeg进行解码\n",
    "    image_contents = tf.read_file(input_queue[0])\n",
    "    image = tf.image.decode_jpeg(image_contents, channels=3)\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, image_W, image_H)\n",
    "    image = tf.image.per_image_standerdization(image)\n",
    "    \n",
    "    image_batch, label_batch = tf.train.batch([image, label], \n",
    "        batch_size = batch_size,\n",
    "        num_threads = 16,\n",
    "        capacity = capacity)\n",
    "    \n",
    "    label_batch = tf.reshape(label_batch, [batch_size])\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "CAPACITY = 64\n",
    "IMG_W = 208\n",
    "IMG_H = 208\n",
    "\n",
    "# 进行训练的目录\n",
    "train_dir = \"/home/wilson/workstation/TFResFaceEmotion/emotions\"\n",
    "\n",
    "# 获取原始图片信息列表\n",
    "image_list, label_list = get_files(train_dir)\n",
    "print(\"image_list:\", len(image_list))\n",
    "print(\"label_list:\", len(label_list))\n",
    "\n",
    "image_batch,label_batch = get_batch(image_list,label_list,IMG_W,IMG_H,BATCH_SIZE,CAPACITY)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    i = 0\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord = coord)\n",
    "    try:\n",
    "        while not coord.should_stop() and i<2:\n",
    "            # 提取出2个batch的图片进行可视化\n",
    "            img, label = sess.run([image_batch, label_batch])\n",
    "            for j in np.arrange(BATCH_SIZE):\n",
    "                print('label:%d' %label[j])\n",
    "                plt.imshow(img[j,:,:,:])\n",
    "                plt.show()\n",
    "            i+=1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('done!')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
