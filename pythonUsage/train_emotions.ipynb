{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "#图像读取库\n",
    "from PIL import Image\n",
    "#矩阵运算库\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'happy', 'sad']\n",
      "{'anger': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'sad': 4}\n"
     ]
    }
   ],
   "source": [
    "# 分类\n",
    "label_names = [\"anger\", \"disgust\", \"fear\", \"happy\", \"sad\"]\n",
    "label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
    "\n",
    "print(label_names)\n",
    "print(label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练的图片列表和分类标签信息\n",
    "def load_images(data_dir):\n",
    "    fpaths = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in os.listdir(data_dir):\n",
    "        labelPath = os.path.join(data_dir, label)\n",
    "        for fname in os.listdir(labelPath):\n",
    "            # 文件名\n",
    "            fpath = os.path.join(labelPath, fname)\n",
    "            fpaths.append(fpath)\n",
    "            \n",
    "            # 标签\n",
    "            labels.append(label_to_index[label])\n",
    "    \n",
    "    # 将图片名和标签 打包成一组，然后进行乱序\n",
    "    temp = np.array([fpaths,labels])\n",
    "    temp = temp.transpose()\n",
    "    np.random.shuffle(temp)\n",
    "\n",
    "    fpaths = list(temp[:,0])\n",
    "    labels = list(temp[:,1])\n",
    "    labels = [int(i) for i in labels]\n",
    "    \n",
    "    return fpaths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 79 images\n"
     ]
    }
   ],
   "source": [
    "# 数据文件夹\n",
    "data_dir = \"/home/wilson/workstation/TFResFaceEmotion/emotions\"\n",
    "\n",
    "all_image_paths, all_image_labels = load_images(data_dir)\n",
    "\n",
    "image_count = len(all_image_paths)\n",
    "print(\"total\", image_count, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image,dtype=tf.float32)\n",
    "    image /= 255.0\n",
    "    #print(333, image.shape, image)\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = all_image_paths[0]\n",
    "label = all_image_labels[0]\n",
    "\n",
    "plt.imshow(load_and_preprocess_image(image_path))\n",
    "plt.grid(False)\n",
    "plt.title(label_names[label].title())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANcAAADXCAYAAACJfcS1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAACoklEQVR4nO3YMW6DQBRFUSZiH9n/stxnDz995BTIvobE55QwxWuuGLFmZgOe7+PsAfBfiQsi4oKIuCAiLoiICyL7kcNrLf/t4YeZWfee+3JBRFwQERdExAURcUFEXBARF0TEBRFxQURcEBEXRMQFEXFBRFwQERdExAURcUFEXBARF0TEBRFxQURcEBEXRMQFEXFBRFwQERdExAURcUFEXBARF0TEBRFxQURcEBEXRMQFEXFBRFwQERdExAURcUFEXBARF0TEBRFxQURcEBEXRMQFEXFBRFwQERdExAURcUFEXBARF0TEBRFxQURcEBEXRMQFEXFBRFwQERdExAURcUFEXBARF0TEBRFxQURcEBEXRMQFEXFBRFwQERdExAURcUFEXBARF0TEBRFxQURcEBEXRMQFEXFBRFwQERdExAURcUFEXBARF0TEBRFxQURcD1pnD+CyxPWgOXsAlyUuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6IiAsi4oKIuCAiLoiICyLigoi4ICIuiIgLIuKCiLggIi6I7AfPf23bdiuGwB/1+duLNTOvHAJvw7UQIuKCiLggIi6IiAsi4oKIuCAiLoiICyLf39cOl6cfg5oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABUUlEQVR4nO3WuwkDMRBAwZNxC47df1nOr4d1bpwc+PuYCRcFCw8JrZnZ6Dh9ewFeS9AYQWMEjRE05nzk8FrLl/h37DNzeRy6of/r9mwoaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaMz54Pl927bbOxbhsOuz4ZqZTy/CG3lyYwSNETRG0BhBYwSNETRG0BhBY+57+w6yhMa/JQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABXElEQVR4nO3dMQqDMBiAUVO8Qufe/1jdvUO6FxexpfbjvfEngcCH4hTHnHOh4/brA/BZgsYIGiNojKAx65HFYwyfxNexzTnv70NP6P967g0FjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtAYQWMEjRE0RtALGif2Choj6AWdua5N0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRG0BhBYwSNETRmPbh+W5bl+Y2DcNhjbzjmPPMfH67GKzdG0BhBYwSNETRG0BhBYwSNETTmBXXDD7KBsVTOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABUUlEQVR4nO3WuwkDMRBAwZNxC47df1nOr4d1bpwc+PuYCRcFCw8JrZnZ6Dh9ewFeS9AYQWMEjRE05nzk8FrLl/h37DNzeRy6of/r9mwoaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaIygMYLGCBojaMz54Pl927bbOxbhsOuz4ZqZTy/CG3lyYwSNETRG0BhBYwSNETRG0BhBY+57+w6yhMa/JQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for n, image in enumerate(image_ds.take(4)):\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.imshow(image)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "anger\n",
      "anger\n",
      "fear\n",
      "anger\n",
      "anger\n",
      "fear\n",
      "happy\n",
      "anger\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64))\n",
    "\n",
    "for label in label_ds.take(10):\n",
    "    print(label_names[label.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ZipDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n",
      "<MapDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# 由于这些数据集顺序相同，你可以将他们打包在一起得到一个(图片, 标签)对数据集：\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "print(image_label_ds)\n",
    "\n",
    "# 元组被解压缩到映射函数的位置参数中\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "    return load_and_preprocess_image(path), label\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "image_label_ds = ds.map(load_and_preprocess_from_path_label)\n",
    "print(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "<PrefetchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"训练的基本方法\n",
    "要使用此数据集训练模型，你将会想要数据：\n",
    "\n",
    "    被充分打乱。\n",
    "    被分割为 batch。\n",
    "    永远重复。\n",
    "    尽快提供 batch。\n",
    "\n",
    "使用 tf.data api 可以轻松添加这些功能。\n",
    "\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 设置一个和数据集大小一致的 shuffle buffer size（随机缓冲区大小）以保证数据\n",
    "# 被充分打乱。\n",
    "ds = image_label_ds.shuffle(buffer_size=image_count)\n",
    "ds = ds.repeat()\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "# 当模型在训练的时候，`prefetch` 使数据集在后台取得 batch。\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "print(ds)\n",
    "\n",
    "ds = image_label_ds.apply(\n",
    "  tf.data.experimental.shuffle_and_repeat(buffer_size=image_count))\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在准备分类模型\n",
    "mobile_net = tf.keras.applications.MobileNetV2(input_shape=(192, 192, 3), include_top=False)\n",
    "mobile_net.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该模型期望它的输出被标准化至 [-1,1] 范围内：\n",
    "# help(tf.keras.applications.mobilenet_v2.preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 6, 6, 1280)\n"
     ]
    }
   ],
   "source": [
    "# 在你将输出传递给 MobilNet 模型之前，你需要将其范围从 [0,1] 转化为 [-1,1]：\n",
    "def change_range(image,label):\n",
    "    return 2*image-1, label\n",
    "\n",
    "keras_ds = ds.map(change_range)\n",
    "\n",
    "# 数据集可能需要几秒来启动，因为要填满其随机缓冲区。\n",
    "image_batch, label_batch = next(iter(keras_ds))\n",
    "\n",
    "feature_map_batch = mobile_net(image_batch)\n",
    "print(feature_map_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型保存位置\n",
    "model_save_path = 'emotion.model'\n",
    "\n",
    "# 模型\n",
    "model = None\n",
    "if os.path.exists(model_save_path):\n",
    "    model = keras.models.load_model(model_save_path)\n",
    "else:\n",
    "    model = tf.keras.Sequential([\n",
    "        mobile_net,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(label_names), activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min logit: 0.054046214\n",
      "max logit: 0.4372469\n",
      "\n",
      "Shape: (32, 5)\n"
     ]
    }
   ],
   "source": [
    "logit_batch = model(image_batch).numpy()\n",
    "\n",
    "print(\"min logit:\", logit_batch.min())\n",
    "print(\"max logit:\", logit_batch.max())\n",
    "print()\n",
    "\n",
    "print(\"Shape:\", logit_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_192 (Functi (None, 6, 6, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch=tf.math.ceil(len(all_image_paths)/BATCH_SIZE).numpy()\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "# 清除旧的log\n",
    "# !rm -rf ./logs/\n",
    "\n",
    "# 日志统计回调\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    2/30000 [..............................] - ETA: 15:04 - loss: 1.1189 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0207s vs `on_train_batch_end` time: 0.0393s). Check your callbacks.\n",
      "30000/30000 [==============================] - 580s 19ms/step - loss: 1.0229 - accuracy: 0.6682\n",
      "Epoch 2/10\n",
      "30000/30000 [==============================] - 582s 19ms/step - loss: 0.8968 - accuracy: 0.7327\n",
      "Epoch 3/10\n",
      "30000/30000 [==============================] - 578s 19ms/step - loss: 0.8045 - accuracy: 0.7675\n",
      "Epoch 4/10\n",
      "30000/30000 [==============================] - 582s 19ms/step - loss: 0.7311 - accuracy: 0.7911\n",
      "Epoch 5/10\n",
      "30000/30000 [==============================] - 582s 19ms/step - loss: 0.6697 - accuracy: 0.8225\n",
      "Epoch 6/10\n",
      "30000/30000 [==============================] - 582s 19ms/step - loss: 0.6168 - accuracy: 0.8559\n",
      "Epoch 7/10\n",
      "30000/30000 [==============================] - 582s 19ms/step - loss: 0.5699 - accuracy: 0.8785\n",
      "Epoch 8/10\n",
      "30000/30000 [==============================] - 577s 19ms/step - loss: 0.5279 - accuracy: 0.8979\n",
      "Epoch 9/10\n",
      "30000/30000 [==============================] - 573s 19ms/step - loss: 0.4900 - accuracy: 0.9159\n",
      "Epoch 10/10\n",
      "30000/30000 [==============================] - 573s 19ms/step - loss: 0.4556 - accuracy: 0.9317\n",
      "train done.\n",
      "INFO:tensorflow:Assets written to: emotion.model/assets\n",
      "model saved to emotion.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5015), started 2:08:48 ago. (Use '!kill 5015' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-46f874d81fc5fcf2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-46f874d81fc5fcf2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 执行训练\n",
    "model.fit(ds, epochs=10, steps_per_epoch=30000, \n",
    "         callbacks=[tensorboard_callback])\n",
    "print(\"train done.\")\n",
    "\n",
    "# 保存模型\n",
    "model.save(model_save_path)\n",
    "print(\"model saved to\", model_save_path)\n",
    "\n",
    "# %load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用命令转换 模型到tfile\n",
    "# 下列命令参数用于输入和输出文件。\n",
    "#    --output_file. 类型: string. 指定输出文件的绝对路径。\n",
    "#    --saved_model_dir. 类型: string. 指定含有 TensorFlow 1.x 或者 2.0 使用 SavedModel 生成文件的绝对路径目录。\n",
    "#    --keras_model_file. Type: string. 指定含有 TensorFlow 1.x 或者 2.0 使用 tf.keras model 生成 HDF5 文件的绝对路径目录。\n",
    "#例如： tflite_convert --saved_model_dir=./emotion.model --output_file=./emotion.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
